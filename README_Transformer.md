# æ—¶åºæ•°æ®é¢„æµ‹ç³»ç»Ÿ - å®Œæ•´æ–‡æ¡£

æœ¬é¡¹ç›®å®ç°äº†ä¸‰ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ç”¨äºåŸºäºä¼ æ„Ÿå™¨æ•°æ®é¢„æµ‹äººä½“å…³èŠ‚åŠ›çŸ©ï¼š
1. **TCN** - æ—¶é—´å·ç§¯ç½‘ç»œ
2. **Transformeré¢„æµ‹æ¨¡å‹** - åŸºäºEncoderçš„ç›´æ¥é¢„æµ‹
3. **Transformerç”Ÿæˆæ¨¡å‹** - åŸºäºEncoder-Decoderçš„è‡ªå›å½’ç”Ÿæˆ

## ğŸ“ é¡¹ç›®ç»“æ„

```
project/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default_config.py          # ç»Ÿä¸€é…ç½®æ–‡ä»¶ï¼ˆæ”¯æŒæ‰€æœ‰æ¨¡å‹ï¼‰
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tcn.py                      # TCNæ¨¡å‹
â”‚   â”œâ”€â”€ predictor_model.py          # Transformeré¢„æµ‹æ¨¡å‹
â”‚   â”œâ”€â”€ generative_model.py         # Transformerç”Ÿæˆæ¨¡å‹
â”‚   â””â”€â”€ positional_encoding.py     # ä½ç½®ç¼–ç 
â”œâ”€â”€ dataset_loaders/
â”‚   â”œâ”€â”€ dataloader.py               # TCNæ•°æ®åŠ è½½å™¨
â”‚   â””â”€â”€ sequence_dataloader.py     # Transformeræ•°æ®åŠ è½½å™¨
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ config_utils.py             # é…ç½®åŠ è½½å·¥å…·
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/                      # è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ test/                       # æµ‹è¯•æ•°æ®
â”œâ”€â”€ logs/                           # è®­ç»ƒæ—¥å¿—å’Œæ¨¡å‹ä¿å­˜
â”œâ”€â”€ train.py                        # ç»Ÿä¸€è®­ç»ƒè„šæœ¬
â””â”€â”€ test.py                         # ç»Ÿä¸€æµ‹è¯•è„šæœ¬
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
pip install torch pandas numpy
```

### 2. å‡†å¤‡æ•°æ®

æ•°æ®ç»“æ„ï¼š
```
data/
â”œâ”€â”€ train/
â”‚   â””â”€â”€ å‚ä¸è€…ID/
â”‚       â””â”€â”€ è¿åŠ¨ç±»å‹/
â”‚           â”œâ”€â”€ å‚ä¸è€…ID_è¿åŠ¨ç±»å‹_exo.csv          # ä¼ æ„Ÿå™¨æ•°æ®
â”‚           â””â”€â”€ å‚ä¸è€…ID_è¿åŠ¨ç±»å‹_moment_filt.csv  # åŠ›çŸ©çœŸå€¼
â””â”€â”€ test/
    â””â”€â”€ ï¼ˆç›¸åŒç»“æ„ï¼‰
```

### 3. é…ç½®æ¨¡å‹

ç¼–è¾‘ `configs/default_config.py`ï¼š

```python
# é€‰æ‹©æ¨¡å‹ç±»å‹
model_type = 'GenerativeTransformer'  # 'TCN', 'Transformer', æˆ– 'GenerativeTransformer'

# æ ¹æ®æ¨¡å‹ç±»å‹é…ç½®ç›¸åº”å‚æ•°...
```

### 4. è®­ç»ƒ

```bash
# åŸºç¡€è®­ç»ƒ
python train.py --device cuda

# æŒ‡å®šé…ç½®
python train.py --config_path configs.my_config --device cuda --num_workers 4

# æ¢å¤è®­ç»ƒ
python train.py --resume logs/trained_*/*/model_epoch_100.tar
```

### 5. æµ‹è¯•

```bash
# åŸºç¡€æµ‹è¯•
python test.py --model_path logs/trained_*/*/best_model.tar --device cuda

# ç”Ÿæˆå¼æ¨¡å‹ä½¿ç”¨è‡ªå›å½’ç”Ÿæˆ
python test.py --model_path path/to/model.tar --use_generation
```

## ğŸ”¬ ä¸‰ç§æ¨¡å‹è¯¦è§£

### 1. TCN (Temporal Convolutional Network)

**æ¶æ„ç‰¹ç‚¹ï¼š**
- å› æœå·ç§¯ï¼Œä½¿ç”¨å®Œæ•´åºåˆ—å†å²
- è†¨èƒ€å·ç§¯æ‰©å¤§æ„Ÿå—é‡
- æ®‹å·®è¿æ¥

**ä¼˜ç‚¹ï¼š**
- é€‚åˆå®æ—¶åœ¨çº¿é¢„æµ‹
- å†…å­˜æ•ˆç‡é«˜
- ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯

**ç¼ºç‚¹ï¼š**
- æ„Ÿå—é‡å—é™
- è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢
- éœ€è¦paddingå¤„ç†å˜é•¿åºåˆ—

**é…ç½®ç¤ºä¾‹ï¼š**
```python
model_type = 'TCN'
num_channels = [64, 64]
ksize = 3
eff_hist = 248
batch_size = 4
```

**ä½¿ç”¨åœºæ™¯ï¼š**
- å®æ—¶é¢„æµ‹ç³»ç»Ÿ
- åºåˆ—é•¿åº¦å˜åŒ–å¤§
- éœ€è¦å®Œæ•´å†å²ä¿¡æ¯

---

### 2. Transformeré¢„æµ‹æ¨¡å‹

**æ¶æ„ç‰¹ç‚¹ï¼š**
```
è¾“å…¥ [B,C,N] â†’ å½’ä¸€åŒ– â†’ æŠ•å½±åˆ°d_model â†’ ä½ç½®ç¼–ç  
â†’ Transformer Encoder â†’ è¾“å‡ºæŠ•å½± â†’ é¢„æµ‹ [B,output_size,N]
```

**ä¼˜ç‚¹ï¼š**
- å¹¶è¡Œè®¡ç®—æ•ˆç‡é«˜
- å…¨å±€æ³¨æ„åŠ›æœºåˆ¶
- è®­ç»ƒé€Ÿåº¦å¿«

**ç¼ºç‚¹ï¼š**
- å›ºå®šé•¿åº¦çª—å£
- å†…å­˜å ç”¨è¾ƒå¤§

**é…ç½®ç¤ºä¾‹ï¼š**
```python
model_type = 'Transformer'
sequence_length = 100
d_model = 128
nhead = 8
num_encoder_layers = 4
batch_size = 32
```

**ä½¿ç”¨åœºæ™¯ï¼š**
- ç¦»çº¿æ‰¹é‡é¢„æµ‹
- æ•°æ®é‡å……è¶³
- éœ€è¦å¿«é€Ÿè®­ç»ƒ

---

### 3. Transformerç”Ÿæˆæ¨¡å‹ â­ æ–°å¢

**æ¶æ„ç‰¹ç‚¹ï¼š**
```
ä¼ æ„Ÿå™¨æ•°æ® [B,C,N]:
  â†’ å½’ä¸€åŒ– â†’ æŠ•å½± â†’ Encoderç‰¹å¾ [B,N,d_model]
                          â†“ (ä½œä¸ºmemory)
åŠ›çŸ©æ•°æ® [B,2,N]:              â†“
  â†’ Shifted â†’ æŠ•å½± â†’ Decoder â†’ Cross-Attention
                          â†“
                    è¾“å‡º [B,2,N]
```

**ç¼–ç å™¨é€‰é¡¹ï¼š**
1. **Transformer Encoder** (`encoder_type='transformer'`)
   - ä½¿ç”¨å¤šå±‚Transformer Encoderå¤„ç†ä¼ æ„Ÿå™¨æ•°æ®
   - æ•æ‰è¾“å…¥åºåˆ—çš„å¤æ‚æ—¶åºå…³ç³»
   - å‚æ•°æ›´å¤šï¼Œè¡¨è¾¾èƒ½åŠ›æ›´å¼º

2. **Linear Encoder** (`encoder_type='linear'`)
   - ä½¿ç”¨ç®€å•çš„çº¿æ€§å±‚å¤„ç†ä¼ æ„Ÿå™¨æ•°æ®
   - è½»é‡çº§ï¼Œè®­ç»ƒæ›´å¿«
   - é€‚åˆè¾“å…¥ç‰¹å¾ç›¸å¯¹ç®€å•çš„åœºæ™¯

**è®­ç»ƒæ¨¡å¼ï¼š**
- ä½¿ç”¨Teacher Forcing
- è§£ç å™¨è¾“å…¥ï¼šå³ç§»çš„çœŸå®åŠ›çŸ©å€¼
- ä½¿ç”¨å› æœæ©ç é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯

**æµ‹è¯•æ¨¡å¼ï¼š**
- è‡ªå›å½’ç”Ÿæˆ
- ä»èµ·å§‹tokenå¼€å§‹é€æ­¥é¢„æµ‹
- æ¯æ­¥çš„é¢„æµ‹ä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥

**é…ç½®ç¤ºä¾‹ï¼š**
```python
model_type = 'GenerativeTransformer'
encoder_type = 'transformer'  # æˆ– 'linear'
gen_sequence_length = 100
gen_d_model = 128
gen_nhead = 8
gen_num_encoder_layers = 3
gen_num_decoder_layers = 3
start_token_value = 0.0
teacher_forcing_ratio = 1.0
```

**ä½¿ç”¨åœºæ™¯ï¼š**
- éœ€è¦åºåˆ—åˆ°åºåˆ—å»ºæ¨¡
- æƒ³è¦åˆ©ç”¨è§£ç å™¨çš„è‡ªå›å½’ç‰¹æ€§
- å¯¹ç”Ÿæˆè´¨é‡æœ‰è¾ƒé«˜è¦æ±‚

**æµ‹è¯•é€‰é¡¹ï¼š**
```bash
# Teacher Forcingæ¨¡å¼ï¼ˆå¿«é€Ÿï¼Œç”¨äºè¯„ä¼°ï¼‰
python test.py --model_path path/to/model.tar

# è‡ªå›å½’ç”Ÿæˆæ¨¡å¼ï¼ˆçœŸå®åœºæ™¯ï¼‰
python test.py --model_path path/to/model.tar --use_generation
```

---

## ğŸ“Š æ¨¡å‹å¯¹æ¯”

| ç‰¹æ€§ | TCN | Transformeré¢„æµ‹ | Transformerç”Ÿæˆ |
|------|-----|----------------|----------------|
| æ¶æ„ | å·ç§¯ | Encoder | Encoder-Decoder |
| åºåˆ—é•¿åº¦ | å˜é•¿ | å›ºå®šçª—å£ | å›ºå®šçª—å£ |
| è®¡ç®—æ¨¡å¼ | åºåˆ— | å¹¶è¡Œ | éƒ¨åˆ†å¹¶è¡Œ |
| è®­ç»ƒé€Ÿåº¦ | æ…¢ | å¿« | ä¸­ç­‰ |
| å†…å­˜ä½¿ç”¨ | ä½ | ä¸­ | é«˜ |
| æ‰¹æ¬¡å¤§å° | 2-8 | 16-64 | 16-32 |
| å®æ—¶æ€§ | ä¼˜ç§€ | è‰¯å¥½ | è‰¯å¥½ |
| é¢„æµ‹æ–¹å¼ | ç›´æ¥ | ç›´æ¥ | è‡ªå›å½’ |
| å‚æ•°é‡ | ä¸­ | ä¸­ | å¤§ |

## âš™ï¸ é…ç½®å‚æ•°è¯¦è§£

### é€šç”¨å‚æ•°

```python
# æ•°æ®é…ç½®
data_dir = 'data'
side = "r"  # 'l' æˆ– 'r'
model_delays = [10, 0]  # æ¯ä¸ªè¾“å‡ºçš„å»¶è¿Ÿ

# è®­ç»ƒé…ç½®
num_epochs = 1000
batch_size = 32  # æ ¹æ®æ¨¡å‹è°ƒæ•´
learning_rate = 0.001
weight_decay = 1e-5
random_seed = 42

# å½’ä¸€åŒ–å‚æ•°ï¼ˆéœ€è¦ä»è®­ç»ƒæ•°æ®è®¡ç®—ï¼‰
center = torch.tensor([...])
scale = torch.tensor([...])
```

### TCNä¸“ç”¨å‚æ•°

```python
num_channels = [64, 64]      # å„å±‚é€šé“æ•°
ksize = 3                     # å·ç§¯æ ¸å¤§å°
eff_hist = 248                # æœ‰æ•ˆå†å²é•¿åº¦
dropout = 0.2                 # Dropoutæ¯”ç‡
spatial_dropout = False       # æ˜¯å¦ä½¿ç”¨ç©ºé—´dropout
activation = 'ReLU'           # æ¿€æ´»å‡½æ•°
norm = 'weight_norm'          # å½’ä¸€åŒ–æ–¹æ³•
```

### Transformeré¢„æµ‹æ¨¡å‹å‚æ•°

```python
sequence_length = 100         # åºåˆ—çª—å£é•¿åº¦
d_model = 128                 # æ¨¡å‹ç»´åº¦
nhead = 8                     # æ³¨æ„åŠ›å¤´æ•°
num_encoder_layers = 4        # Encoderå±‚æ•°
dim_feedforward = 512         # FFNç»´åº¦
transformer_dropout = 0.1     # Dropoutæ¯”ç‡
use_positional_encoding = True # æ˜¯å¦ä½¿ç”¨ä½ç½®ç¼–ç 
```

### Transformerç”Ÿæˆæ¨¡å‹å‚æ•°

```python
encoder_type = 'transformer'  # ç¼–ç å™¨ç±»å‹
gen_sequence_length = 100     # åºåˆ—é•¿åº¦
gen_d_model = 128             # æ¨¡å‹ç»´åº¦
gen_nhead = 8                 # æ³¨æ„åŠ›å¤´æ•°
gen_num_encoder_layers = 3    # Encoderå±‚æ•°
gen_num_decoder_layers = 3    # Decoderå±‚æ•°
gen_dim_feedforward = 512     # FFNç»´åº¦
gen_dropout = 0.1             # Dropoutæ¯”ç‡
start_token_value = 0.0       # èµ·å§‹tokenå€¼
teacher_forcing_ratio = 1.0   # Teacher forcingæ¯”ç‡
```

## ğŸ¯ è¶…å‚æ•°è°ƒä¼˜å»ºè®®

### TCNè°ƒä¼˜

| å‚æ•° | æ¨èèŒƒå›´ | å½±å“ |
|------|---------|------|
| num_channels | [32,32] ~ [128,128] | æ¨¡å‹å®¹é‡ |
| ksize | 3-7 | æ„Ÿå—é‡å¤§å° |
| dropout | 0.1-0.3 | è¿‡æ‹Ÿåˆæ§åˆ¶ |
| batch_size | 2-8 | å†…å­˜/ç¨³å®šæ€§ |

### Transformeré¢„æµ‹æ¨¡å‹è°ƒä¼˜

| å‚æ•° | æ¨èèŒƒå›´ | å½±å“ |
|------|---------|------|
| sequence_length | 50-200 | ä¸Šä¸‹æ–‡é•¿åº¦ |
| d_model | 64-256 | è¡¨è¾¾èƒ½åŠ› |
| nhead | 4-16 | æ³¨æ„åŠ›å¤šæ ·æ€§ |
| num_encoder_layers | 2-8 | æ¨¡å‹æ·±åº¦ |
| batch_size | 16-64 | è®­ç»ƒæ•ˆç‡ |

### Transformerç”Ÿæˆæ¨¡å‹è°ƒä¼˜

| å‚æ•° | æ¨èèŒƒå›´ | å½±å“ |
|------|---------|------|
| encoder_type | transformer/linear | ç¼–ç å™¨å¤æ‚åº¦ |
| gen_num_encoder_layers | 2-6 | ç¼–ç æ·±åº¦ |
| gen_num_decoder_layers | 2-6 | è§£ç æ·±åº¦ |
| gen_d_model | 64-256 | æ¨¡å‹å®¹é‡ |
| batch_size | 16-32 | å†…å­˜å¹³è¡¡ |

## ğŸ“ˆ è®­ç»ƒæŠ€å·§

### 1. æ•°æ®é¢„å¤„ç†

```python
# è®¡ç®—å½’ä¸€åŒ–å‚æ•°ï¼ˆä»è®­ç»ƒæ•°æ®ï¼‰
# ç¡®ä¿centerå’Œscaleæ˜¯åŸºäºå¹²å‡€æ•°æ®ï¼ˆæ— NaNï¼‰è®¡ç®—çš„
center = train_data.mean(axis=0, keepdims=True)
scale = train_data.std(axis=0, keepdims=True)
```

### 2. å­¦ä¹ ç‡è°ƒåº¦

```python
# é…ç½®ReduceLROnPlateau
scheduler_factor = 0.9        # è¡°å‡å› å­
scheduler_patience = 10       # è€å¿ƒå€¼
min_lr = 1e-6                 # æœ€å°å­¦ä¹ ç‡
```

### 3. æ—©åœç­–ç•¥

```python
early_stopping_patience = 50  # è€å¿ƒå€¼
early_stopping_min_delta = 0.0 # æœ€å°æ”¹å–„
```

### 4. æ¢¯åº¦è£å‰ª

```python
grad_clip = 1.0  # é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
```

## ğŸ› å¸¸è§é—®é¢˜

### 1. è®­ç»ƒæŸå¤±ä¸ºNaN

**åŸå› ï¼š**
- æ•°æ®ä¸­æœ‰NaNå€¼
- å­¦ä¹ ç‡è¿‡å¤§
- center/scaleå‚æ•°é”™è¯¯

**è§£å†³ï¼š**
```python
# ç¡®ä¿centerå’Œscaleæ­£ç¡®
# é™ä½å­¦ä¹ ç‡
learning_rate = 0.0001
# NaNä¼šè‡ªåŠ¨è¿‡æ»¤ï¼Œä½†ç¡®ä¿å½’ä¸€åŒ–å‚æ•°æ­£ç¡®
```

### 2. å†…å­˜ä¸è¶³

**è§£å†³ï¼š**
```python
# å‡å°batch_size
batch_size = 16  # æˆ–æ›´å°

# å‡å°åºåˆ—é•¿åº¦ï¼ˆTransformerï¼‰
sequence_length = 50

# å‡å°æ¨¡å‹å¤§å°
d_model = 64
num_encoder_layers = 2
```

### 3. ç”Ÿæˆæ¨¡å‹é¢„æµ‹è´¨é‡å·®

**å¯èƒ½åŸå› ï¼š**
- Teacher forcing ratioè®¾ç½®ä¸å½“
- èµ·å§‹tokené€‰æ‹©ä¸åˆç†
- è§£ç å™¨å±‚æ•°ä¸è¶³

**è§£å†³ï¼š**
```python
# è°ƒæ•´teacher forcing
teacher_forcing_ratio = 0.8  # é€æ¸é™ä½

# é€‰æ‹©åˆé€‚çš„èµ·å§‹token
start_token_value = 0.0  # æˆ–æ•°æ®çš„å‡å€¼

# å¢åŠ è§£ç å™¨æ·±åº¦
gen_num_decoder_layers = 4
```

### 4. æ¨¡å‹é€‰æ‹©å»ºè®®

**é€‰æ‹©TCNï¼š**
- âœ… éœ€è¦å®æ—¶é¢„æµ‹
- âœ… åºåˆ—é•¿åº¦å˜åŒ–å¤§
- âœ… å†…å­˜å—é™

**é€‰æ‹©Transformeré¢„æµ‹ï¼š**
- âœ… ç¦»çº¿æ‰¹é‡å¤„ç†
- âœ… æ•°æ®é‡å……è¶³
- âœ… éœ€è¦å¿«é€Ÿè®­ç»ƒ

**é€‰æ‹©Transformerç”Ÿæˆï¼š**
- âœ… éœ€è¦åºåˆ—å»ºæ¨¡
- âœ… å¯¹ç”Ÿæˆè´¨é‡è¦æ±‚é«˜
- âœ… æƒ³æ¢ç´¢è‡ªå›å½’æ–¹æ³•

## ğŸ“ æ€§èƒ½æŒ‡æ ‡

### è¯„ä¼°æŒ‡æ ‡

1. **RMSE (Root Mean Square Error)**
   - å•ä½ï¼šNm/kg
   - è¶Šå°è¶Šå¥½
   - < 0.05 ä¸ºä¼˜ç§€

2. **RÂ² (R-squared)**
   - èŒƒå›´ï¼š-âˆ to 1
   - è¶Šæ¥è¿‘1è¶Šå¥½
   - \> 0.90 ä¸ºä¼˜ç§€

3. **å½’ä¸€åŒ–MAE (Normalized Mean Absolute Error)**
   - èŒƒå›´ï¼š0 to 1
   - è¶Šå°è¶Šå¥½
   - < 0.02 ä¸ºä¼˜ç§€

## ğŸ”¬ è¿›é˜¶ä½¿ç”¨

### è‡ªå®šä¹‰ç¼–ç å™¨ç±»å‹

```python
# åœ¨generative_model.pyä¸­
# å°è¯•ä¸åŒçš„ç¼–ç å™¨æ¶æ„

# 1. çº¯çº¿æ€§ç¼–ç å™¨ï¼ˆæœ€å¿«ï¼‰
encoder_type = 'linear'

# 2. Transformerç¼–ç å™¨ï¼ˆæœ€å¼ºï¼‰
encoder_type = 'transformer'
gen_num_encoder_layers = 4
```

### æ··åˆè®­ç»ƒç­–ç•¥

```python
# å¯ä»¥å°è¯•è¯¾ç¨‹å­¦ä¹ 
# å¼€å§‹æ—¶ä½¿ç”¨teacher_forcing_ratio=1.0
# é€æ¸é™ä½åˆ°0.5
```

### é›†æˆå­¦ä¹ 

```python
# è®­ç»ƒå¤šä¸ªä¸åŒé…ç½®çš„æ¨¡å‹
# ä½¿ç”¨åŠ æƒå¹³å‡è¿›è¡Œé¢„æµ‹
predictions = 0.4 * model1_pred + 0.3 * model2_pred + 0.3 * model3_pred
```

## ğŸ“š å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬é¡¹ç›®ï¼Œè¯·å¼•ç”¨ï¼š
- **Transformer:** "Attention Is All You Need" (Vaswani et al., 2017)
- **TCN:** "An Empirical Evaluation of Generic Convolutional and Recurrent Networks" (Bai et al., 2018)
- **åº”ç”¨èƒŒæ™¯:** "Task-Agnostic Exoskeleton Control via Biological Joint Moment Estimation"

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤Issueã€‚

## ğŸ“„ è®¸å¯è¯

MIT License